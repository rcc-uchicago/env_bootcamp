{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curve Fitting\n",
    "\n",
    "In the class on interpolation, we wanted to find a function that passes through each of our sampled data points. This was typically defined piecewise over our sampled data. Something similar we might want to do is find a single function of a given form that comes closest to our sampled data, but doesn't necessarily pass through it (and usually won't)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Least Squares Fitting\n",
    "\n",
    "Say we have sampled 10 points, and want to find the equation of the straight line that passes through all 10 points. The equation of a line has two parameters: $y = a_0 + a_1 x$. Each sampled data point can be used to give us an equation that could be used to find what the parameters of a straight line passing through that point are. We only have 2 parameters, but will obtain 10 equations from our sampled points. This means that our system is overdetermined. The chance that 10 sampled points will fall on the same line is effectively zero unless they have been chosen to do so (and so are not in fact sampled), so there is no way to solve this exactly.\n",
    "\n",
    "Instead, we want to do the best we can and find the line that passes most closely to each of the sampled points. There are a number of ways we could choose to do this. The most commonly used approach is to find the deviation from the line at each of our sampled points, and find the set of parameters that minimize the total of the squares of these deviations, hence the term least squares fitting. This particular approach was first used by Gauss in determining the orbits of comets, and he showed the the least squares estimate coincides with maximum likelihood estimates for independent normally distributed errors.\n",
    "\n",
    "Let's express what we want to do mathematically:\n",
    "- We have some set of $M$ sampled points $(x_i, y_i)$.\n",
    "- We have some function we want to fit to our data we'll call $F(x)$\n",
    "- $F(x)$ is expressed in terms of some set of $N$ parameters $\\{a_j\\}$. For example a line would be $a_0 + a_1 x$, with $N=2$. So let's denote this as $F(x; \\{a_j\\})$.\n",
    "- For sampled point $i$, denote the distance between the point and the line as $\\Delta i = F(x; \\{a_j\\}) - y_i$\n",
    "- We want to find the $\\{a_j\\}$ that will minimize the the following:\n",
    "$$ S(\\{a_j\\}) = \\Sigma_{i=1}^M \\Delta i^2 = \\Sigma_{i=1}^M \\left[F(x_i; \\{a_j\\}) - y_i\\right]^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try and do this with Python, starting from some random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's create some M random data points as we did for the interpolation\n",
    "M = 5\n",
    "x_data = np.linspace(0, M-1, M)\n",
    "# Generate 11 random y-values between 0 and 1\n",
    "y_data = np.random.rand(M)\n",
    "# To get something that looks more well behaved lets sort the\n",
    "# y-values\n",
    "y_data.sort()\n",
    "\n",
    "plt.plot(x_data, y_data, 'bo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we want to find the linear function that best fits this data. Our function is now $F(x; \\{a_j\\}) = a_0 + a_1 x$. We want to minimize the function $D(a_0, a_1) = \\Sigma_{i=1}^M (a_0 + a_1 x_i - y_i)^2$. At the minimum, we'll have\n",
    "$$ \\frac{\\partial D}{\\partial a_0} = 0 = 2 \\Sigma_{i=1}^M (a_0 + a_1 x_i - y_i) $$\n",
    "and\n",
    "$$ \\frac{\\partial D}{\\partial a_1} = 0 = 2 \\Sigma_{i=1}^M (a_0 + a_1 x_i - y_i) x_i. $$\n",
    "When we fill in our x and y values in the expression for $D$, we'll have two equations in two unknowns ($a_0$ and $a_1$). This is a linear system which we have already learned how to solve.\n",
    "\n",
    "Gathering the coefficients and simplifying (leaving out the summation limits to declutter) we get\n",
    "$$ M a_0 + a_1 \\Sigma x_i = \\Sigma y_i $$\n",
    "$$ a_0 \\Sigma x_i + a_1 \\Sigma x_i^2 = \\Sigma x_i y_i $$\n",
    "\n",
    "(Recall $M$ is our number of sampled points). This would straight-forward to solve by hand, but let's use the numpy.linalg.solve function to find the coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recall we know how to solve the linear system Ax=b\n",
    "# Let's construct our problem matrix and solution vector\n",
    "A = np.array([[M, x_data.sum()],[x_data.sum(), np.square(x_data).sum()]])\n",
    "b = np.array([y_data.sum(), np.dot(x_data, y_data)])\n",
    "lin_fit_params = np.linalg.solve(A, b)\n",
    "print(\"a_0 =\", lin_fit_params[0], \", a_1 =\", lin_fit_params[1])\n",
    "\n",
    "# Now let's plot this along with our sampled data\n",
    "fit_vals = np.array(lin_fit_params[0] + lin_fit_params[1]*x_data)\n",
    "plt.plot(x_data, y_data, 'bo', x_data, fit_vals, 'r-',)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like a pretty good fit overall. You can try regenerating the random data and also changing the number of sampled points and see how the fit looks each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Linear Least Squares Fitting\n",
    "\n",
    "Let's see how we might do this for a general function. Let's say we can write the function we wish to fit to our data as\n",
    "$$ F(x, {a_j}) = a_0 F_0(x) + a_1 F_1(x) + a_3 F_2(x) + \\cdots a_N F_N(x) = \\Sigma_{j=1}^N a_j F_j(x) $$\n",
    "For example, an $N^{\\mathrm{th}}$-order polynomial fit could have $F_i(x)=x^i$.\n",
    "\n",
    "Again, we want to minimize $D({a_j}) = \\Sigma_{i=1}^M [\\Sigma_{j=1}^N a_j F_j(x_i) - y_i]^2$. At the minimum the partial derivative with respect to each $a_j$ will be zero, so:\n",
    "$$  \\Sigma_{i=1}^M F_j(x_i) [\\Sigma_{k=1}^N a_k F_k(x_i) - y_i] = 0,$$\n",
    "which we can rewrite as\n",
    "$$ \\Sigma_{i=1}^M \\Sigma_{k=1}^N F_j(x_i) F_k(x_i) a_k = \\Sigma_{i=1}^N F_j(x_i) y_i $$\n",
    "\n",
    "Lets express this in matrix form by first defining a matrix $\\mathbf{X}$ containing our various $F_j$ evaluated at each of our $x_i$ as\n",
    "$$ \\mathbf{X} = \\begin{pmatrix} F_1(x_1) & F_2 (x_1) & \\cdots \\\\ F_1(x_2) & F_2(x_2) & \\cdots \\\\\n",
    "\\vdots & \\vdots & \\ddots \\end{pmatrix}. $$\n",
    "The equation we wish to solve for $\\mathbf{a}=(a_0, a_1, \\cdots)$, and $\\mathbf{b}=(y_0, y_1, \\cdots)$ is then\n",
    "$$ (\\mathbf{X}^\\mathrm{T}\\mathbf{X})\\mathbf{a} = \\mathbf{X}^\\mathrm{T}\\mathbf{b}. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's use this to write a function that does an Nth order polynomial fit to some input data\n",
    "\n",
    "def polyfit(data, n):\n",
    "    '''\n",
    "    Nth order polynomial least squares fit\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : (M,2) array\n",
    "        Array of (x,y) values of the data to fit\n",
    "    n : integer\n",
    "        The order of the polynomial to fit. E.g n=1 is linear\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    a : array\n",
    "        The n+1 parameters of the polynomial fit.\n",
    "        A linear fit y=a0+a1x would return (a0, a1).\n",
    "    '''\n",
    "    # First let's construct the transpose of our X array. \n",
    "    # This is a little easier than generating X directly.\n",
    "    # We'll make a list of rows and covert that to a numpy array.\n",
    "    XT = np.array([data[:, 0] ** i for i in range(n + 1)])\n",
    "    return np.linalg.solve(np.dot(XT, XT.T), np.dot(XT, data[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# It'll be useful to have a function that will return the value of\n",
    "# the polynomial defined by our set of parameters at a given x.\n",
    "def polyfunc(params, x):\n",
    "    '''The value of a polynomial defined by the set of params'''\n",
    "    xpowers = np.array([x ** i for i in range(len(params))])\n",
    "    return np.dot(params, xpowers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We need to first get our data in the form expected by our function.\n",
    "# Of course we could also have written our function to take the x and y\n",
    "# values as separate arrays.\n",
    "data = np.array([x_data, y_data]).T\n",
    "\n",
    "# Do the least sqaures fit for a cubic polynomial\n",
    "poly_params = polyfit(data, 3)\n",
    "print(poly_params)\n",
    "\n",
    "#Let's generate a set of 100 points in the same range as our data for plotting\n",
    "x_vals = np.linspace(0, M-1, 100)\n",
    "y_vals = polyfunc(poly_params, x_vals)\n",
    "plt.plot(x_data, y_data, 'bo', x_vals, y_vals, 'r-',)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the number of parameters in our polynomial fit is equal to the number of data points, we'll get the interpolating polynomial. If this is what we actually want, it could be calculated more efficiently with a different approach as we saw in the seciton on interpolation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-polynomial fits and linearization\n",
    "\n",
    "While we've only considered polynomials so far, the $F_i$ above can in pricple take any form for this linear least squares approach. For example, we could fit $y=a_0 + a_1x^{-1}$ just as easily. As long as you can write your total function as a sum of some fit parameters times some functions, then you can use this approach to find the optimal fit parameters.\n",
    "\n",
    "Many other cases, while not immediately falling into this category, can be linearized such that this approach will work. For example, say you want to fit an exponential model to your data:\n",
    "\n",
    "$$\n",
    "y=a_0 \\mathrm e^{a_1 x}\n",
    "$$\n",
    "\n",
    "where $a_0$ and $a_1$ are your fit parameters. Clearly in it's current form this equation can't be fit with the type of approach we've looked at. However if we take the log of both sides we find\n",
    "\n",
    "$$\n",
    "\\ln y = \\ln a_0 + a_1\\ln \\mathrm e = \\ln a_0 + a_1.\n",
    "$$\n",
    "\n",
    "So if we instead fit a straight line to $(x, \\ln y)$, we can find the optimal values of $a_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Errors\n",
    "\n",
    "While simply plotting the curve along with the data is usually a good way to tell if the obtained fit is reasonable, you may want to be able to say something more scientific about you good your fit is, and find the statistical error associated with your fit parameters.\n",
    "\n",
    "In our least squares approach as outlined about we minimized the sum of the square of the residuals:\n",
    "$$ S(\\{a_j\\}) = \\Sigma_{i=1}^M \\Delta_i^2 = \\Sigma_{i=1}^M \\left[F(x_i; \\{a_j\\}) - y_i\\right]^2. $$\n",
    "\n",
    "We can evaluate this with our final fit parameters and use it to estimate the standard error of the fit to $M$ data points with $N+1$ parameters as\n",
    "$$ s = \\sqrt{\\frac{S}{M-N-1}}, $$\n",
    "where the denominator is the number of degrees of freedom in our fit. This gives an estimate of how much the data spreads from our fit curve.\n",
    "\n",
    "It can also be shown that the diagonal and off-diagonal elements of the matrix given by $(\\mathbf X^T\\mathbf X)^{-1}$ can be used to find the variances and covariances respectively of the fit parameters $a_i$.\n",
    "\n",
    " - The variance of a random variable $X$ is the difference between the expectation of the value squared and the square of the expectation of the value: $var(X) = E(X^2)-E(X)^2$. This gives an estimate of the size of its spread. The standard deviation is the square root of the variance.\n",
    " - The covariance of a pair of random variables $X$ and $Y$ is given by $cov(X, Y) = E(XY) - E(Y)E(Y)$. This can be used to estimate the dependency of $X$ on $Y$. If $cov(X, Y) = 0$ the variables are completely independent of each other.\n",
    " \n",
    "Let's denote the elements of $(\\mathbf X^T\\mathbf X)^{-1}$ as $\\epsilon_{ij}$. Then we can write (with $i, j$ starting from 1, but $a_0$ is our first $a$).\n",
    "\n",
    "$$\n",
    "var(a_{i-1})=\\epsilon_{i,i}s^2\n",
    "$$\n",
    "\n",
    "which can be used to find the standard deviation associated with the fit parameter, and\n",
    "\n",
    "$$\n",
    "cov(a_{i-1}, a_{j-1})=\\epsilon_{i,j}s^2.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Inversion\n",
    "\n",
    "Now that we've expressed what we want to find, the one remaining thing we have not yet discussed is how to find the matrix inverse. In general, to invert a matrix $M$, we want to find the matrix $M^{-1}$ such that $MM^{-1}=I$, where $I$ is the identity matrix.\n",
    "\n",
    "Say we have a two by two matrix, we could write this as\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix} m_{11} & m_{12} \\\\ m_{21} & m_{22} \\end{pmatrix}\n",
    "\\begin{pmatrix} m_{11}' & m_{12}' \\\\ m_{21}' & m_{22}' \\end{pmatrix} =\n",
    "\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix},\n",
    "$$\n",
    "\n",
    "where we want to solve for $m_{ij}'$. We can break this down and solve for one column of $M^{-1}$ at a time, giving us 2 linear systems to solve here, but $n$ for an $n\\times n$ matrix.\n",
    "E.g the first would be:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix} m_{11} & m_{12} \\\\ m_{21} & m_{22} \\end{pmatrix}\n",
    "\\begin{pmatrix} m_{11}' \\\\ m_{21}'  \\end{pmatrix} =\n",
    "\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "So we have a linear system, of the type $\\mathbf M \\mathbf x = \\mathbf b$, where we want to solve for a set of different $\\mathbf b$ but the same $\\mathbf M$. We saw in an earlier section that LU decomposition gives a good general way to solve this set of systems and find the elements of the inverse.\n",
    "\n",
    "This is exactly how the the [scipy.linalg.inv](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.inv.html) function operates (via an interface to LAPACK routines that actually do the work). Note that special types of matrices can have other approaches that are more optimal for computing an inverse, but there is no numpy or scipy function that calls these LAPACK routines implementing these methods currently. New features are added to SciPy all the time, so if you do know your matrix may have a better algorithm for inversion and it's large enough that it may be a time consuming part of your code I suggest double checking the SciPy documentation, or considering building an interface to a more optimal method.\n",
    "\n",
    "Note while you may be tempted to solve a linear system $\\mathbf M \\mathbf x = \\mathbf b$ by calculating $\\mathbf M^{-1}$ and evaluating $\\mathbf x = \\mathbf M^{-1}\\mathbf b$, it should be clear that this is a very sub-optimal approach as you will end up solving $N$ linear systems to invert the matrix, and accruing all the numerical errors associated with that, rather than solving the single linear system you had to begin with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing This In SciPy\n",
    "\n",
    "The function [scipy.stats.linregress](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html) can be used to find a linear regression (fit) for a set of x and y values. This will also return several statistical quantities.\n",
    "\n",
    "[numpy.polyfit](https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html) will perform a polynomial fit through a set of data, and return an array of the coefficients. Note the coefficents are in the reverse order to the function we generated above. This uses [numpy.linalg.lstsq](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.lstsq.html) which computes the general linear least squares as outlined above. There is an optional input that can be set in `np.polyfit` to return the residuals and covariance matrix that could be used to evaluate the standard deviation of the fit parameters. The array of coefficients returned by `np.polyfit()` can be passed to [numpy.polyval](https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyval.html) to evaluate the fit at a point or array of points.\n",
    "\n",
    "Most of the fitting functions in [scipy.optimize](https://docs.scipy.org/doc/scipy/reference/optimize.html) in fact use the non-linear approach we'll outline shortly, except for [scipy.optimize.lsq_linear](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.lsq_linear.html) which can use the approach outlined here, but has many additional options and can be used to solve a linear least squares problem with bounds on the variables, and can also be used with sparse matrices. This may be useful if you want to constrain some of your fit parameters to be within some set of physical or measured bounds. This function does not take a function as an argument. Rather it takes a matrix $A$ which is equivalent to the matrix $\\mathbf X$ we defined earlier in describing the general least squares problem, and vector $b$ composed of the y-values as we had."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's reproduce our fit with np.polyfit.\n",
    "nppoly_params = np.polyfit(x_data, y_data, deg=3)\n",
    "# We're using the dense x_vals list we generated earlier.\n",
    "np_y_vals = np.polyval(nppoly_params, x_vals)\n",
    "\n",
    "plt.plot(x_data, y_data, 'bo', label=\"data\")\n",
    "plt.plot(x_vals, y_vals, 'r-', label=\"our function\")\n",
    "plt.plot(x_vals, np_y_vals, 'g--', label=\"numpy.polyfit\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linear Least Squares\n",
    "\n",
    "There are many functions you might want to fit to your data which don't fall into this category and can't be linearized such that they can be made to work. So we'll look at a different approach that can be used for these functions. Our goal is the same however: we want to determine some set of function parameters that will **minimize the sum of the squares of the residuals**. The usual approach here is iterative rather than simply solving a matrix equation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing a function of several variables\n",
    "\n",
    "There are many algorithms for performing this minimization, and some of these methods can also be applied to the general problem of finding the minimum of a function of many variables. This can be quite a tricky problem, as it can be easy to become trapped in a local minimum and never reach the global minimum.\n",
    "\n",
    "Let's start with one of the simplest ones:\n",
    "\n",
    "#### Gradient Descent\n",
    "\n",
    "Say we want to find the minimum of $F(\\mathbf x)$.\n",
    "- Start from an initial guess $\\mathbf x_0$.\n",
    "- Iterate to find values of $\\mathbf x_n$ using the procedure $\\mathbf x_{n+1} = \\mathbf x_n -h_n\\nabla F(\\mathbf x_n)$, where $h_n$ is some step size.\n",
    "- Since we likely want to take smaller steps as we approach the minimum, $h_n$ must be chosen carefully.\n",
    "- One approach for evaluting $h_n$ is given by\n",
    "\n",
    "$$\n",
    "h_n  =\\frac{(x_n - x_{n-1})^T\\left[\\nabla F(x_n) - \\nabla F(x_{n-1})\\right]}\n",
    "{\\left|\\left|\\nabla F(x_n)-\\nabla F(x_{n-1})\\right|\\right|^2}.\n",
    "$$\n",
    "\n",
    "So this approach essentially says: to find the minimum take a downhill jump of a size that gets smaller as we seem to be approaching the minimum.\n",
    "\n",
    "If we set $F(\\mathbf x)$ as a function giving the sum of the squares of the residuals, we could then apply this process to finding optimal fit parameters.\n",
    "\n",
    "#### Gauss-Newton algorithm\n",
    "\n",
    "This is a generalization of Newton's method, that can be used to minimize a sum of squared function values. It proceeds as follows.\n",
    "\n",
    "- We start from some initial guess for our function parameters.\n",
    "- We can use a Taylor expansion to approximate our non-linear function in an approximate linear form.\n",
    "- Then we can use linear least squares to obtain a new guess for our parameters with a smaller set of residuals.\n",
    "\n",
    "Say we have a nonlinear function $f(x, \\{a_j\\})$ and we want to find the $\\{a_j\\}$ that such that the sum of the squares of the residuals between our data and our function is minimized. For each of our $x_i$ points, and given some set of parameter guesses $\\{a_j^{(n)}\\}$, by truncating the Taylor expansion to linear order we can write:\n",
    "\n",
    "$$\n",
    "f(x_i)_{n+1} = f(x_i)_n + \\Sigma_k \\frac{\\partial f(x_i)_n}{\\partial a_k}\\Delta a_k\n",
    "$$\n",
    "\n",
    "where $\\Delta a_k = a_k^{(n+1)} - a_k^{(n)}$ and $f(x_i)_n = f(x_i, \\{a_j^{(n)}\\})$.\n",
    "\n",
    "We can then express what we want to do in matrix form as\n",
    "\n",
    "$$\n",
    " \\begin{pmatrix}\n",
    " \\frac{\\partial f(x_1)}{\\partial a_0} & \\frac{\\partial f(x_1)}{\\partial a_1} & \\cdots & \\frac{\\partial f(x_1)}{\\partial a_N} \\\\\n",
    " \\frac{\\partial f(x_2)}{\\partial a_0} & \\frac{\\partial f(x_2)}{\\partial a_1} & \\cdots & \\frac{\\partial f(x_2)}{\\partial a_N} \\\\\n",
    "  \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "  \\frac{\\partial f(x_M)}{\\partial a_0} & \\frac{\\partial f(x_M)}{\\partial a_1} & \\cdots & \\frac{\\partial f(x_M)}{\\partial a_N} \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix} \\Delta a_0 \\\\ \\Delta a_1 \\\\ \\vdots \\\\ \\Delta a_N \\end{pmatrix} =\n",
    "\\begin{pmatrix} y_1 - f(x_1) \\\\ y_2 -f(x_2) \\\\ \\vdots \\\\ y_M -f(x_M) \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "We can apply linear least squares to this equation to find the best values of $\\Delta a_j$. So we can iterate this process, with each step giving us some correction to our initial guess $ \\{ a_j^{(0)}\\}$. This can be repeated until the solution converges within some tolerance. If the partial derivative is difficult to evaluate, it can be approximated by taking some small perturbation to $a_k$.\n",
    "\n",
    "On top of the problem of potentially getting stuck in a local minimum, this may also have convergence issues, particularly if you start from an initial guess that's very far from a minimum.\n",
    "\n",
    "#### Levenbergâ€“Marquardt algorithm\n",
    "\n",
    "This is the approach used by most curve fitting software, including the MINPACK Fortran library functions called by the SciPy curve fitting routines. It is an interpolation between the gradient descent and Gauss-Newton algorithms outlined here, and is more robust than the latter; it can converge to a minimum even if the initial guess is far from the minimum. Details of this algorithm can be seen [here](https://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm). As with the Gauss-Newton algorithm, this is approach minimizes the sum of squared function values, so is most applicable to the non-linear least squares problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In SciPy\n",
    "\n",
    "As mentioned earlier, most of the fitting functions in [scipy.optimize](https://docs.scipy.org/doc/scipy/reference/optimize.html) can be used to fit nonlinear functions. You'll see listed [scipy.optimize.leastsq](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.leastsq.html) and [scipy.optimize.least_squares](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.least_squares.html), which are both wrappers for MINPACK library functions and essentially do the same thing. `leastsq` is on older version of the non-linear least squares fitting implementation that has been retained for backwards compatibility. The `least_squares` function is a more recent addition that does the same thing, but has many more options to tune the fit (such as bound the fit parameters) and method used.\n",
    "\n",
    "There is also [scipy.optimize.curve_fit](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html) which is somewhat more user friendly if you just want to fit a function.\n",
    "\n",
    "Let's go through an example of how we might use this to fit a [Birch-Murnaghan](https://en.wikipedia.org/wiki/Birch%E2%80%93Murnaghan_equation_of_state) equation of state to some DFT calculated data of total energy (Ry) vs volume (Bohr^3) in silicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First let's load our data and plot it.\n",
    "si_EvV = np.loadtxt(\"etot_v_vol.dat\")\n",
    "plt.plot(si_EvV[:, 0], si_EvV[:, 1], 'bo')\n",
    "plt.xlabel(\"Volume (Bohr^3)\")\n",
    "plt.ylabel(\"Total Energy (Ry)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's define a function that returns E(V) with\n",
    "# some additional fit parameters\n",
    "def BirchMurnaghanE(V, E0, V0, B0, BP):\n",
    "    '''Birch-Murnaghan Equation of State for E(V).'''\n",
    "    E = E0 + 9*V0*B0/16 * (BP*((V0/V)**(2/3) - 1)**3 +\n",
    "            ((V0/V)**(2/3) - 1)**2 * (6-4*(V0/V)**(2/3)))\n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We'll need some initial guesses for the fit.\n",
    "# Generating a guess for E0 and V0 from the plot or data\n",
    "# will be sufficient, we can put 1 for the other two parameters.\n",
    "BM_guess = [si_EvV[:, 1].min(), si_EvV[:, 0][np.argmin(si_EvV[:, 1])], 1, 1]\n",
    "\n",
    "# And now Call the curve_fit function\n",
    "BMp, cov = scipy.optimize.curve_fit(BirchMurnaghanE, si_EvV[:, 0],si_EvV[:, 1], BM_guess)\n",
    "\n",
    "print(\"E0 = \", BMp[0])\n",
    "print(\"V0 = \", BMp[1])\n",
    "print(\"B0 = \", BMp[2])\n",
    "print(\"BP = \", BMp[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xplt = np.linspace(si_EvV[:, 0].min(), si_EvV[:, 0].max(), 100)\n",
    "yplt = BirchMurnaghanE(xplt, *BMp)\n",
    "plt.plot(si_EvV[:, 0], si_EvV[:, 1], 'bo')\n",
    "plt.plot(xplt, yplt, 'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "We touched a little on minimization of functions in more than one dimension above. We won't have time to go into this in detail. In a general optimization problem we want to find a maxima or minima of a multidimensional function.\n",
    "\n",
    "Think back to **Newton's method** as we covered in an earlier section. In this method we use the function and it's derivative to find where the function was zero. We can modify this if we want to find the minima or maxima of a function, by searching instead for where the first derivative of the function is zero, with the iteration now becoming:\n",
    "\n",
    "$$\n",
    "x_{n+1} = x_n - \\frac{f'(x_n)}{f''(x_n)}.\n",
    "$$\n",
    "\n",
    "We can generalize this to a higher dimensional function $f(\\mathbf x)$, by replacing the derivative with the gradient $\\nabla f(\\mathbf x)$, and the inverse of the second derivative with the inverse of the Hessian matrix $\\mathbf H f(\\mathbf x)$. So this now becomes \n",
    "\n",
    "$$\n",
    "\\mathbf x_{n+1} = \\mathbf x_n - h[\\mathbf H f(\\mathbf x_n)]^{-1} \\nabla f'(\\mathbf x_n).\n",
    "$$\n",
    "where $h$ is often uses to scale the step size and help ensure convergence.\n",
    "\n",
    "### Quasi-Newton methods\n",
    "\n",
    "A big problem with the above approach is the need to calculate the Hessian and inverting it at each step. Quasi-Newton methods avoid the need for calculating the Hessian. It is instead approximated from successive gradient vectors instead. This approach is effectively a generalization of the secant method. There are many flavours of quasi-Newton methods.\n",
    "\n",
    "### In SciPy\n",
    "The main optimization function available in [scipy.optimize](https://docs.scipy.org/doc/scipy/reference/optimize.html) is the [scipy.optimize.minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) function. This implements many different minimization approaches as options, including the quasi-Newton method \"BFGS\" which is often used in electronic structure codes from optimizing crystal structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multi_f(x):\n",
    "    return np.sin(x[0]) * np.cos(x[1])\n",
    "\n",
    "# We can give this the function, and initial guess, and the method.\n",
    "scipy.optimize.minimize(multi_f, x0=[0.0, 0.0], method='BFGS')\n",
    "\n",
    "# It will return an object with various info about the optmization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
